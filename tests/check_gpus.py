#!/usr/bin/env python3
"""
GPUæ£€æŸ¥å·¥å…·
ç”¨äºæŸ¥çœ‹å½“å‰æœåŠ¡å™¨çš„GPUé…ç½®å’Œå¯ç”¨æ€§
"""

import torch
import sys
import os
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

try:
    import sys
    import os
    sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'training'))
    from train_qwen_multi_gpu import GPUManager
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥GPUManagerï¼Œè¯·ç¡®ä¿train_qwen_multi_gpu.pyå­˜åœ¨")
    sys.exit(1)


def print_separator(title="", char="=", width=60):
    """æ‰“å°åˆ†éš”çº¿"""
    if title:
        title = f" {title} "
        padding = (width - len(title)) // 2
        line = char * padding + title + char * (width - padding - len(title))
    else:
        line = char * width
    print(line)


def check_cuda_environment():
    """æ£€æŸ¥CUDAç¯å¢ƒ"""
    print_separator("CUDAç¯å¢ƒæ£€æŸ¥")
    
    print(f"ğŸ Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
    print(f"ğŸ”¥ PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    if torch.cuda.is_available():
        print(f"âœ… CUDAå¯ç”¨: {torch.version.cuda}")
        print(f"ğŸ¯ cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        return False
    
    return True


def check_gpu_details():
    """æ£€æŸ¥GPUè¯¦ç»†ä¿¡æ¯"""
    print_separator("GPUè¯¦ç»†ä¿¡æ¯")
    
    if not torch.cuda.is_available():
        print("âŒ æ²¡æœ‰å¯ç”¨çš„GPU")
        return
    
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ”¢ GPUæ€»æ•°: {gpu_count}")
    print()
    
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        memory_total = props.total_memory / (1024**3)  # GB
        
        # å®‰å…¨åœ°è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
        try:
            memory_allocated = torch.cuda.memory_allocated(i) / (1024**3)
            memory_reserved = torch.cuda.memory_reserved(i) / (1024**3)
            memory_free = memory_total - memory_reserved
        except Exception:
            # å¦‚æœæ— æ³•è·å–å†…å­˜ä¿¡æ¯ï¼Œä½¿ç”¨æ€»å†…å­˜ä½œä¸ºå¯ç”¨å†…å­˜
            memory_allocated = 0.0
            memory_reserved = 0.0
            memory_free = memory_total
        
        print(f"ğŸ“± GPU {i}: {props.name}")
        print(f"   ğŸ’¾ æ€»å†…å­˜: {memory_total:.1f} GB")
        print(f"   ğŸŸ¢ å¯ç”¨å†…å­˜: {memory_free:.1f} GB")
        print(f"   ğŸ”´ å·²ç”¨å†…å­˜: {memory_reserved:.1f} GB")
        print(f"   âš¡ è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        print(f"   ğŸ”§ å¤šå¤„ç†å™¨: {props.multi_processor_count}")
        print()


def check_gpu_manager():
    """æ£€æŸ¥GPUç®¡ç†å™¨åŠŸèƒ½"""
    print_separator("GPUç®¡ç†å™¨æ£€æŸ¥")
    
    try:
        # è·å–GPUä¿¡æ¯
        gpu_info = GPUManager.get_available_gpus()
        
        print(f"ğŸ¯ å¯ç”¨GPU: {gpu_info['available']}")
        
        if 'A800' in gpu_info and gpu_info['A800']:
            print(f"ğŸš€ A800 GPU: {gpu_info['A800']}")
        
        if 'A40' in gpu_info and gpu_info['A40']:
            print(f"âš¡ A40 GPU: {gpu_info['A40']}")
        
        if 'other' in gpu_info and gpu_info['other']:
            print(f"ğŸ”§ å…¶ä»–GPU: {gpu_info['other']}")
        
        print()
        
        # æµ‹è¯•GPUé€‰æ‹©åŠŸèƒ½
        print("ğŸ§ª æµ‹è¯•GPUé€‰æ‹©åŠŸèƒ½:")
        
        # è‡ªåŠ¨é€‰æ‹©
        try:
            auto_gpus = GPUManager.select_gpus()
            print(f"   è‡ªåŠ¨é€‰æ‹©: {auto_gpus}")
        except Exception as e:
            print(f"   è‡ªåŠ¨é€‰æ‹©å¤±è´¥: {e}")
        
        # æŒ‰ç±»å‹é€‰æ‹©A800
        try:
            a800_gpus = GPUManager.select_gpus(gpu_type="A800")
            print(f"   A800é€‰æ‹©: {a800_gpus}")
        except Exception as e:
            print(f"   A800é€‰æ‹©å¤±è´¥: {e}")
        
        # æŒ‰ç±»å‹é€‰æ‹©A40
        try:
            a40_gpus = GPUManager.select_gpus(gpu_type="A40")
            print(f"   A40é€‰æ‹©: {a40_gpus}")
        except Exception as e:
            print(f"   A40é€‰æ‹©å¤±è´¥: {e}")
        
        # æ‰‹åŠ¨é€‰æ‹©
        try:
            manual_gpus = GPUManager.select_gpus(gpu_ids=[0, 1])
            print(f"   æ‰‹åŠ¨é€‰æ‹©[0,1]: {manual_gpus}")
        except Exception as e:
            print(f"   æ‰‹åŠ¨é€‰æ‹©å¤±è´¥: {e}")
            
    except Exception as e:
        print(f"âŒ GPUç®¡ç†å™¨æ£€æŸ¥å¤±è´¥: {e}")


def show_usage_examples():
    """æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹"""
    print_separator("ä½¿ç”¨ç¤ºä¾‹")
    
    print("ğŸš€ å¯åŠ¨è„šæœ¬æ–¹å¼:")
    print("   # è‡ªåŠ¨é€‰æ‹©GPU")
    print("   ./run_training.sh 1 auto my_experiment")
    print()
    print("   # æŒ‡å®šGPUç±»å‹")
    print("   ./run_training.sh 1 A800 my_experiment")
    print()
    print("   # æ‰‹åŠ¨é€‰æ‹©GPU")
    print("   ./run_training.sh 1 manual my_experiment \"0,1,2,3\"")
    print()
    
    print("ğŸ Pythonè„šæœ¬æ–¹å¼:")
    print("   # æŒ‡å®šGPU ID")
    print("   python train_qwen_multi_gpu.py --gpu_ids 0 1 2 3")
    print()
    print("   # æŒ‡å®šGPUç±»å‹")
    print("   python train_qwen_multi_gpu.py --gpu_type A800")
    print()
    print("   # å•GPUè®­ç»ƒ")
    print("   python train_qwen_multi_gpu.py --gpu_ids 0")
    print()


def show_recommendations():
    """æ˜¾ç¤ºæ¨èé…ç½®"""
    print_separator("æ¨èé…ç½®")
    
    if not torch.cuda.is_available():
        print("âŒ æ— GPUå¯ç”¨ï¼Œæ— æ³•æä¾›æ¨è")
        return
    
    try:
        gpu_info = GPUManager.get_available_gpus()
        
        print("ğŸ’¡ æ ¹æ®å½“å‰GPUé…ç½®çš„æ¨è:")
        print()
        
        if 'A800' in gpu_info and gpu_info['A800']:
            print(f"ğŸš€ å¤§æ¨¡å‹è®­ç»ƒ (æ¨èA800): GPU {gpu_info['A800']}")
            print("   ./run_training.sh 1 A800 large_model_experiment")
            print()
        
        if 'A40' in gpu_info and gpu_info['A40']:
            print(f"âš¡ æ ‡å‡†è®­ç»ƒ (æ¨èA40): GPU {gpu_info['A40']}")
            print("   ./run_training.sh 1 A40 standard_experiment")
            print()
        
        if len(gpu_info['available']) >= 4:
            print("ğŸ”¥ å¤šGPUå¹¶è¡Œè®­ç»ƒ:")
            first_four = gpu_info['available'][:4]
            print(f"   ./run_training.sh 1 manual multi_gpu_experiment \"{','.join(map(str, first_four))}\"")
            print()
        
        print("ğŸ§ª è°ƒè¯•/æµ‹è¯• (å•GPU):")
        print(f"   ./run_training.sh 1 manual debug_experiment \"{gpu_info['available'][0]}\"")
        print()
        
    except Exception as e:
        print(f"âŒ æ— æ³•ç”Ÿæˆæ¨è: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” GPUæ£€æŸ¥å·¥å…·")
    print_separator()
    
    # æ£€æŸ¥CUDAç¯å¢ƒ
    if not check_cuda_environment():
        print("\nâŒ CUDAç¯å¢ƒä¸å¯ç”¨ï¼Œæ— æ³•ç»§ç»­æ£€æŸ¥")
        return 1
    
    print()
    
    # æ£€æŸ¥GPUè¯¦ç»†ä¿¡æ¯
    check_gpu_details()
    
    # æ£€æŸ¥GPUç®¡ç†å™¨
    check_gpu_manager()
    
    print()
    
    # æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹
    show_usage_examples()
    
    # æ˜¾ç¤ºæ¨èé…ç½®
    show_recommendations()
    
    print_separator("æ£€æŸ¥å®Œæˆ")
    print("âœ… GPUæ£€æŸ¥å®Œæˆï¼")
    print("ğŸ“– æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹: README_MIGRATION.md")
    print("ğŸ“‹ é…ç½®ç¤ºä¾‹è¯·æŸ¥çœ‹: training_config_gpu_examples.json")
    
    return 0


if __name__ == "__main__":
    exit(main())