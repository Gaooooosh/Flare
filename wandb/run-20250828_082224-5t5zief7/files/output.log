  0%|                                                                | 0/13500 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Traceback (most recent call last):
  File "/home/xiaoyonggao/Flare/training/train_simple.py", line 330, in <module>
    main()
  File "/home/xiaoyonggao/Flare/training/train_simple.py", line 314, in main
    trainer.train()
  File "/home/xiaoyonggao/Flare/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2238, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/xiaoyonggao/Flare/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2648, in _inner_training_loop
    self.optimizer.step()
  File "/home/xiaoyonggao/Flare/.venv/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
    self.optimizer.step(closure)
  File "/home/xiaoyonggao/Flare/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiaoyonggao/Flare/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 516, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiaoyonggao/Flare/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 81, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiaoyonggao/Flare/.venv/lib/python3.12/site-packages/torch/optim/adam.py", line 237, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/home/xiaoyonggao/Flare/.venv/lib/python3.12/site-packages/torch/optim/adam.py", line 177, in _init_group
    state["exp_avg"] = torch.zeros_like(
                       ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 1 has a total capacity of 44.42 GiB of which 28.50 MiB is free. Process 1951723 has 36.78 GiB memory in use. Including non-PyTorch memory, this process has 7.60 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 258.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
