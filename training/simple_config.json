{
  "model": {
    "model_name": "Qwen/Qwen2.5-3B",
    "max_seq_length": 16384,
    "rope_theta": 1000000.0,
    "no_rope_layers": [
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34
    ],
    "use_flash_attention": true,
    "torch_dtype": "bfloat16"
  },
  "data": {
    "dataset_name": "brando/small-c4-dataset",
    "dataset_config": null,
    "dataset_size": 10000,
    "validation_split": 0.1,
    "max_length": 16384,
    "text_column": "text",
    "cache_dir": "/datacache/huggingface"
  },
  "training": {
    "learning_rate": 0.0001,
    "batch_size": 2,
    "num_epochs": 3,
    "warmup_steps": 100,
    "logging_steps": 10,
    "save_steps": 500,
    "eval_steps": 500
  },
  "environment": {
    "gpu_ids": [
      4
    ],
    "force_cpu": false
  },
  "output": {
    "base_dir": "./output",
    "experiment_name": "somerope_LoRA_20-34"
  }
}