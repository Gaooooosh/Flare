#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä¿®å¤åçš„LoRAè®­ç»ƒ
"""

import os
import sys
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / 'utils'))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, TaskType
from patch_qwen_rope import patch_qwen_rope

def test_fixed_lora():
    """æµ‹è¯•ä¿®å¤åçš„LoRAè®¾ç½®"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤åçš„LoRAè®¾ç½®...")
    
    # ä½¿ç”¨å°æ¨¡å‹
    model_name = "Qwen/Qwen2.5-0.5B"
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
    )
    
    # åº”ç”¨RoPE patch
    print("ğŸ”§ åº”ç”¨RoPE patch...")
    patch_qwen_rope(model, no_rope_layers=[0, 1])
    
    # åº”ç”¨LoRAï¼ˆä¸ä½¿ç”¨prepare_model_for_kbit_trainingï¼‰
    print("ğŸ¯ åº”ç”¨LoRA...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        bias="none",
    )
    model = get_peft_model(model, lora_config)
    
    # æ£€æŸ¥å‚æ•°çŠ¶æ€
    print("\nğŸ“Š æ£€æŸ¥å‚æ•°çŠ¶æ€:")
    trainable_count = 0
    total_count = 0
    lora_count = 0
    
    for name, param in model.named_parameters():
        total_count += 1
        if param.requires_grad:
            trainable_count += 1
            if "lora_" in name:
                lora_count += 1
                print(f"âœ… LoRAå‚æ•°: {name}")
            else:
                print(f"âš ï¸ éLoRAå¯è®­ç»ƒå‚æ•°: {name}")
    
    print(f"\nğŸ“ˆ ç»Ÿè®¡:")
    print(f"æ€»å‚æ•°: {total_count}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_count}")
    print(f"LoRAå‚æ•°: {lora_count}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­å’Œæ¢¯åº¦
    print("\nğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­å’Œæ¢¯åº¦...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    inputs = tokenizer("Hello world", return_tensors="pt", padding=True)
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    try:
        model.train()
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
        print(f"âœ… æŸå¤±è®¡ç®—æˆåŠŸ: {loss.item():.4f}")
        
        if loss.requires_grad:
            loss.backward()
            print("âœ… åå‘ä¼ æ’­æˆåŠŸ")
            
            # æ£€æŸ¥æ¢¯åº¦
            grad_count = 0
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_count += 1
                elif param.requires_grad:
                    print(f"âš ï¸ å‚æ•°éœ€è¦æ¢¯åº¦ä½†æ²¡æœ‰æ¢¯åº¦: {name}")
            
            print(f"ğŸ“ˆ æœ‰æ¢¯åº¦çš„å‚æ•°: {grad_count}")
            return True
        else:
            print("âŒ æŸå¤±ä¸éœ€è¦æ¢¯åº¦ï¼")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_fixed_lora()
    if success:
        print("\nğŸ‰ ä¿®å¤åçš„LoRAæµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nğŸ’¥ ä¿®å¤åçš„LoRAæµ‹è¯•å¤±è´¥ï¼")