#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¢¯åº¦é—®é¢˜è¯Šæ–­è„šæœ¬
"""

import os
import sys
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / 'utils'))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from config_manager import ConfigManager
from patch_qwen_rope import patch_qwen_rope

def debug_model_gradients():
    """è°ƒè¯•æ¨¡å‹æ¢¯åº¦è®¾ç½®"""
    print("ğŸ” å¼€å§‹æ¢¯åº¦è¯Šæ–­...")
    
    # ä½¿ç”¨å°æ¨¡å‹è¿›è¡Œæµ‹è¯•
    model_name = "Qwen/Qwen2.5-0.5B"
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
    )
    
    print("ğŸ”§ åº”ç”¨RoPE patch...")
    patch_qwen_rope(model, no_rope_layers=[0, 1])
    
    print("âš™ï¸ å‡†å¤‡æ¨¡å‹...")
    model = prepare_model_for_kbit_training(model)
    
    print("ğŸ¯ åº”ç”¨LoRA...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        bias="none",
    )
    model = get_peft_model(model, lora_config)
    
    print("\nğŸ“Š æ£€æŸ¥å‚æ•°æ¢¯åº¦è®¾ç½®:")
    trainable_count = 0
    total_count = 0
    
    for name, param in model.named_parameters():
        total_count += 1
        if param.requires_grad:
            trainable_count += 1
            print(f"âœ… {name}: requires_grad={param.requires_grad}, shape={param.shape}")
        else:
            if "lora_" in name:
                print(f"âŒ LoRAå‚æ•°æœªè®¾ç½®æ¢¯åº¦: {name}")
            # else:
            #     print(f"âšª {name}: requires_grad={param.requires_grad}")
    
    print(f"\nğŸ“ˆ ç»Ÿè®¡: {trainable_count}/{total_count} å‚æ•°å¯è®­ç»ƒ")
    
    if trainable_count == 0:
        print("âŒ æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼")
        return False
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\nğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    inputs = tokenizer("Hello world", return_tensors="pt", padding=True)
    # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    try:
        with torch.no_grad():
            outputs = model(**inputs)
        print("âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        
        # æµ‹è¯•æ¢¯åº¦è®¡ç®—
        print("\nğŸ§ª æµ‹è¯•æ¢¯åº¦è®¡ç®—...")
        model.train()
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
        print(f"âœ… æŸå¤±è®¡ç®—æˆåŠŸ: {loss.item():.4f}")
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦éœ€è¦æ¢¯åº¦
        print(f"ğŸ” æŸå¤±æ¢¯åº¦è®¾ç½®: requires_grad={loss.requires_grad}")
        
        if loss.requires_grad:
            loss.backward()
            print("âœ… åå‘ä¼ æ’­æˆåŠŸ")
            
            # æ£€æŸ¥å“ªäº›å‚æ•°æœ‰æ¢¯åº¦
            grad_count = 0
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_count += 1
                elif param.requires_grad:
                    print(f"âš ï¸ å‚æ•°éœ€è¦æ¢¯åº¦ä½†æ²¡æœ‰æ¢¯åº¦: {name}")
            
            print(f"ğŸ“ˆ æœ‰æ¢¯åº¦çš„å‚æ•°: {grad_count}")
            
        else:
            print("âŒ æŸå¤±ä¸éœ€è¦æ¢¯åº¦ï¼")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True

if __name__ == "__main__":
    success = debug_model_gradients()
    if success:
        print("\nğŸ‰ æ¢¯åº¦è¯Šæ–­é€šè¿‡ï¼")
    else:
        print("\nğŸ’¥ æ¢¯åº¦è¯Šæ–­å¤±è´¥ï¼")