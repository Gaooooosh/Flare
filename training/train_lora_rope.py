#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºLoRAçš„Qwenè®­ç»ƒè„šæœ¬ - æ”¯æŒRoPEå±‚å»é™¤
ä½¿ç”¨PEFTåº“å®ç°é«˜æ•ˆçš„LoRAå¾®è°ƒï¼ŒåŒæ—¶æ”¯æŒæŒ‡å®šå±‚çš„RoPEå»é™¤
"""

import os
import sys
from pathlib import Path

# é…ç½®wandbç½‘ç»œç¯å¢ƒ
os.environ["WANDB_BASE_URL"] = "https://api.bandw.top"

# æ—©æœŸGPUç¯å¢ƒè®¾ç½® - å¿…é¡»åœ¨import torchä¹‹å‰
def setup_gpu_environment():
    """è®¾ç½®GPUç¯å¢ƒå˜é‡ - å¿…é¡»åœ¨import torchä¹‹å‰è°ƒç”¨"""
    config_file = "simple_config.json"
    if Path(config_file).exists():
        # ä¸´æ—¶å¯¼å…¥é…ç½®ç®¡ç†å™¨æ¥è¯»å–GPUè®¾ç½®
        sys.path.append(str(Path(__file__).parent.parent / 'utils'))
        from config_manager import ConfigManager
        
        try:
            config_manager = ConfigManager(config_file)
            config = config_manager.get_config()
            
            if hasattr(config.environment, 'gpu_ids') and config.environment.gpu_ids:
                gpu_ids_str = ",".join(map(str, config.environment.gpu_ids))
                os.environ["CUDA_VISIBLE_DEVICES"] = gpu_ids_str
                print(f"ğŸ¯ è®¾ç½®CUDA_VISIBLE_DEVICES: {gpu_ids_str}")
                # éªŒè¯ç¯å¢ƒå˜é‡è®¾ç½®
                actual_value = os.environ.get("CUDA_VISIBLE_DEVICES", "")
                if actual_value != gpu_ids_str:
                    print(f"âš ï¸ ç¯å¢ƒå˜é‡è®¾ç½®å¼‚å¸¸ï¼ŒæœŸæœ›: {gpu_ids_str}, å®é™…: {actual_value}")
                    raise RuntimeError(f"CUDA_VISIBLE_DEVICESè®¾ç½®å¤±è´¥")
                else:
                    print(f"âœ… CUDA_VISIBLE_DEVICESéªŒè¯æˆåŠŸ: {actual_value}")
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šGPUï¼Œè®¾ç½®ä¸ºä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
                print("ğŸ’» æœªæŒ‡å®šGPUï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU")
        except Exception as e:
            print(f"âš ï¸ è¯»å–GPUé…ç½®å¤±è´¥: {e}")
            raise
    else:
        print("ğŸ“‹ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡GPUç¯å¢ƒè®¾ç½®")

# è®¾ç½®GPUç¯å¢ƒ
setup_gpu_environment()

# ç°åœ¨å¯ä»¥å®‰å…¨å¯¼å…¥torch
import logging
import torch
from typing import Optional, List

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / 'utils'))

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)

# PEFTç›¸å…³å¯¼å…¥
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training,
)

from config_manager import ConfigManager
from patch_qwen_rope import patch_qwen_rope
from simple_dataset_loader import SimpleDatasetLoader

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)


def get_lora_config():
    """è·å–LoRAé…ç½®"""
    return LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=16,  # LoRA rank
        lora_alpha=32,  # LoRA scaling parameter
        lora_dropout=0.1,  # LoRA dropout
        target_modules=[
            "q_proj",
            "k_proj", 
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],  # ç›®æ ‡æ¨¡å—
        bias="none",  # ä¸è®­ç»ƒbias
        use_rslora=False,  # ä¸ä½¿ç”¨RSLoRA
    )


def verify_gradients(model):
    """éªŒè¯æ¨¡å‹å‚æ•°çš„æ¢¯åº¦è®¾ç½®ï¼ˆåªæ£€æŸ¥ï¼Œä¸ä¿®æ”¹ï¼‰"""
    logger.info("éªŒè¯æ¨¡å‹å‚æ•°æ¢¯åº¦è®¾ç½®...")
    
    trainable_params = 0
    total_params = 0
    lora_params = 0
    
    for name, param in model.named_parameters():
        total_params += param.numel()
        
        if param.requires_grad:
            trainable_params += param.numel()
            if "lora_" in name:
                lora_params += param.numel()
                logger.debug(f"LoRAå‚æ•°: {name}")
            else:
                logger.debug(f"å…¶ä»–å¯è®­ç»ƒå‚æ•°: {name}")
    
    logger.info(f"æ€»å‚æ•°: {total_params:,}")
    logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
    logger.info(f"LoRAå‚æ•°: {lora_params:,}")
    
    if trainable_params == 0:
        raise RuntimeError("æ²¡æœ‰æ‰¾åˆ°å¯è®­ç»ƒçš„å‚æ•°ï¼LoRAé…ç½®å¯èƒ½æœ‰é—®é¢˜ã€‚")
    
    return trainable_params, total_params


def setup_model_and_tokenizer(config):
    """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
    logger.info(f"åŠ è½½æ¨¡å‹: {config.model.model_name}")
    
    # å°è¯•åˆå§‹åŒ–CUDAï¼Œå¦‚æœå¤±è´¥åˆ™æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    try:
        # å¼ºåˆ¶åˆå§‹åŒ–CUDAä¸Šä¸‹æ–‡
        if torch.cuda.is_available():
            torch.cuda.init()
            logger.info(f"CUDAåˆå§‹åŒ–æˆåŠŸï¼Œå¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
        else:
            raise RuntimeError("CUDAä¸å¯ç”¨")
    except Exception as e:
        logger.error(f"CUDAåˆå§‹åŒ–å¤±è´¥: {e}")
        print("å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥NVIDIAé©±åŠ¨ç¨‹åºç‰ˆæœ¬æ˜¯å¦ä¸PyTorchå…¼å®¹")
        print("2. å°è¯•é‡æ–°å®‰è£…PyTorch: pip install torch --upgrade")
        print("3. æ£€æŸ¥CUDA_VISIBLE_DEVICESç¯å¢ƒå˜é‡è®¾ç½®")
        print("4. é‡å¯ç³»ç»Ÿæˆ–é‡æ–°åŠ è½½NVIDIAé©±åŠ¨")
        raise RuntimeError("CUDAåˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•è¿›è¡ŒGPUè®­ç»ƒ")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(config.model.model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # GPUæ¨¡å¼è®¾ç½®
    device_map = "auto"
    logger.info("GPUæ¨¡å¼ï¼šä½¿ç”¨auto device_map")
    
    # ç¡®å®šæ•°æ®ç±»å‹
    torch_dtype = getattr(torch, config.model.torch_dtype, torch.bfloat16)
    
    # åŠ è½½æ¨¡å‹
    model_kwargs = {
        "torch_dtype": torch_dtype,
        "device_map": device_map,
        "trust_remote_code": True,
    }
    
    if hasattr(config.model, 'use_flash_attention') and config.model.use_flash_attention:
        model_kwargs["attn_implementation"] = "flash_attention_2"
    
    model = AutoModelForCausalLM.from_pretrained(
        config.model.model_name,
        **model_kwargs
    )

    # è®¾ç½®RoPEå‚æ•°
    if hasattr(config.model, 'rope_theta') and hasattr(model.config, "rope_theta"):
        model.config.rope_theta = config.model.rope_theta
        logger.info(f"è®¾ç½®RoPE thetaä¸º: {config.model.rope_theta}")
    
    # è®¾ç½®æœ€å¤§ä½ç½®åµŒå…¥
    if hasattr(config.model, 'max_position_embeddings'):
        model.config.max_position_embeddings = config.model.max_position_embeddings
        logger.info(f"è®¾ç½®æœ€å¤§ä½ç½®åµŒå…¥ä¸º: {config.model.max_position_embeddings}")
    
    # ä¿å­˜no_rope_layersé…ç½®åˆ°æ¨¡å‹configä¸­
    if hasattr(config.model, 'no_rope_layers'):
        model.config.nope_layers = config.model.no_rope_layers
    
    # åº”ç”¨RoPE patchï¼ˆåœ¨LoRAä¹‹å‰ï¼‰
    if hasattr(config.model, 'no_rope_layers') and config.model.no_rope_layers:
        logger.info(f"ç¦ç”¨RoPEå±‚: {config.model.no_rope_layers}")
        patch_qwen_rope(model, no_rope_layers=config.model.no_rope_layers)
    
    # è·å–LoRAé…ç½®å¹¶åº”ç”¨
    lora_config = get_lora_config()
    logger.info(f"åº”ç”¨LoRAé…ç½®: rank={lora_config.r}, alpha={lora_config.lora_alpha}")
    model = get_peft_model(model, lora_config)
    
    # ç¡®ä¿ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å…¼å®¹ï¼šéœ€è¦è®©è¾“å…¥æ¿€æ´»å‚ä¸è®¡ç®—å›¾ï¼Œå¹¶å…³é—­use_cache
    try:
        if hasattr(model, "enable_input_require_grads"):
            model.enable_input_require_grads()
            logger.info("å·²å¯ç”¨è¾“å…¥requires_gradä»¥å…¼å®¹æ¢¯åº¦æ£€æŸ¥ç‚¹ä¸LoRA")
    except Exception as e:
        logger.warning(f"å¯ç”¨è¾“å…¥requires_gradå¤±è´¥: {e}")

    # è®­ç»ƒæ—¶æ˜ç¡®å…³é—­use_cacheï¼Œé¿å…ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
    try:
        if hasattr(model, "config"):
            model.config.use_cache = False
            logger.info("å·²å°†model.config.use_cacheæ˜¾å¼è®¾ç½®ä¸ºFalse")
    except Exception as e:
        logger.warning(f"è®¾ç½®use_cacheå¤±è´¥: {e}")

    # éªŒè¯æ¢¯åº¦è®¾ç½®ï¼ˆåªæ£€æŸ¥ï¼Œä¸ä¿®æ”¹ï¼‰
    verify_gradients(model)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    model.print_trainable_parameters()
    
    return model, tokenizer


def setup_dataset(config, tokenizer):
    """è®¾ç½®æ•°æ®é›†"""
    logger.info(f"åŠ è½½æ•°æ®é›†: {config.data.dataset_name}")
    
    dataset_loader = SimpleDatasetLoader(
        cache_dir=config.data.cache_dir
    )
    
    train_dataset, eval_dataset = dataset_loader.prepare_dataset(
        dataset_name=config.data.dataset_name,
        tokenizer=tokenizer,
        size_limit=config.data.dataset_size,
        validation_split=config.data.validation_split,
        max_length=config.data.max_length,
        text_column=config.data.text_column
    )
    
    return train_dataset, eval_dataset


def setup_training_args(config, output_dir):
    """è®¾ç½®è®­ç»ƒå‚æ•°"""
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=config.training.num_epochs,
        per_device_train_batch_size=config.training.batch_size,
        per_device_eval_batch_size=config.training.batch_size,
        gradient_accumulation_steps=getattr(config.training, 'gradient_accumulation_steps', 1),
        learning_rate=config.training.learning_rate,
        weight_decay=getattr(config.training, 'weight_decay', 0.01),
        warmup_steps=config.training.warmup_steps,
        logging_steps=config.training.logging_steps,
        save_steps=config.training.save_steps,
        eval_steps=config.training.eval_steps,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to=None,  # ç¦ç”¨wandbç­‰
        dataloader_pin_memory=False,
        dataloader_num_workers=0,
        fp16=getattr(config.training, 'fp16', False),
        bf16=getattr(config.training, 'bf16', True),
        gradient_checkpointing=getattr(config.training, 'gradient_checkpointing', True),
        max_grad_norm=getattr(config.training, 'max_grad_norm', 1.0),
        seed=getattr(config.environment, 'seed', 42),
        data_seed=getattr(config.environment, 'seed', 42),
        remove_unused_columns=True,  # å¯¹LoRAæ›´å®‰å…¨
        # LoRAç‰¹å®šè®¾ç½®
        save_only_model=True,  # åªä¿å­˜LoRAæƒé‡
    )
    
    return training_args


def run_interactive_config():
    """è¿è¡Œäº¤äº’å¼é…ç½®"""
    logger.info("å¯åŠ¨äº¤äº’å¼é…ç½®...")
    import subprocess
    import sys
    
    # è¿è¡Œäº¤äº’å¼é…ç½®è„šæœ¬
    config_script = Path(__file__).parent.parent / "interactive_config.py"
    result = subprocess.run([sys.executable, str(config_script)], 
                          capture_output=False, text=True)
    
    if result.returncode != 0:
        logger.error("äº¤äº’å¼é…ç½®å¤±è´¥")
        return False
    
    logger.info("äº¤äº’å¼é…ç½®å®Œæˆ")
    return True


def main():
    """ä¸»è®­ç»ƒå‡½æ•° - åŸºäºLoRAçš„RoPEå±‚å»é™¤è®­ç»ƒ"""
    
    print("=" * 60)
    print("ğŸš€ Flare LoRA + RoPE è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    print()
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    config_file = "simple_config.json"
    
    if not Path(config_file).exists():
        print("ğŸ“‹ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œå¯åŠ¨äº¤äº’å¼é…ç½®...")
        print()
        if not run_interactive_config():
            print("âŒ é…ç½®å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
            return
        print()
    else:
        print("ğŸ“‹ å‘ç°ç°æœ‰é…ç½®æ–‡ä»¶")
        print("   1. ä½¿ç”¨ç°æœ‰é…ç½®ç»§ç»­è®­ç»ƒ")
        print("   2. é‡æ–°é…ç½®è®­ç»ƒå‚æ•°")
        print()
        
        while True:
            choice = input("è¯·é€‰æ‹© (1/2): ").strip()
            if choice == "1":
                print("âœ… ä½¿ç”¨ç°æœ‰é…ç½®")
                break
            elif choice == "2":
                print("ğŸ”„ å¯åŠ¨äº¤äº’å¼é…ç½®...")
                print()
                if not run_interactive_config():
                    print("âŒ é…ç½®å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
                    return
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")
    
    print()
    print("=" * 60)
    print("ğŸ”§ å¼€å§‹LoRAè®­ç»ƒå‡†å¤‡...")
    print("=" * 60)
    
    # åŠ è½½é…ç½®
    if not Path(config_file).exists():
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        print("âŒ é…ç½®æ–‡ä»¶ä¸¢å¤±ï¼Œè¯·é‡æ–°è¿è¡Œç¨‹åº")
        return
    
    config_manager = ConfigManager(config_file)
    config = config_manager.get_config()
    
    # GPUç¯å¢ƒå·²åœ¨æ–‡ä»¶å¼€å¤´è®¾ç½®
    
    # è®¾ç½®è¾“å‡ºç›®å½• - å¼ºåˆ¶ä½¿ç”¨/work/xiaoyonggaoä½œä¸ºæ ¹ç›®å½•
    base_work_dir = Path("/work/xiaoyonggao")
    if config.output.experiment_name:
        output_dir = base_work_dir / f"{config.output.experiment_name}_lora"
    else:
        output_dir = base_work_dir / "flare_lora_training"
    
    output_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
    print("ğŸ“¦ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆåº”ç”¨LoRAï¼‰...")
    model, tokenizer = setup_model_and_tokenizer(config)
    
    # è®¾ç½®æ•°æ®é›†
    print("ğŸ“Š å‡†å¤‡æ•°æ®é›†...")
    train_dataset, eval_dataset = setup_dataset(config, tokenizer)
    
    # è®¾ç½®è®­ç»ƒå‚æ•°
    print("âš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...")
    training_args = setup_training_args(config, str(output_dir))
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("ğŸ—ï¸ åˆ›å»ºLoRAè®­ç»ƒå™¨...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    print()
    print("=" * 60)
    print("ğŸ¯ å¼€å§‹LoRAè®­ç»ƒ...")
    print("=" * 60)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # ä¿å­˜LoRAæ¨¡å‹
    final_model_dir = output_dir / "final_lora_model"
    print(f"ğŸ’¾ ä¿å­˜LoRAæ¨¡å‹åˆ°: {final_model_dir}")
    trainer.save_model(str(final_model_dir))
    tokenizer.save_pretrained(str(final_model_dir))
    
    # ä¿å­˜å®Œæ•´çš„é…ç½®ä¿¡æ¯
    config_save_path = final_model_dir / "training_config.json"
    config_manager.save_config(str(config_save_path))
    
    print()
    print("=" * 60)
    print("ğŸ‰ LoRAè®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ LoRAæ¨¡å‹ä¿å­˜ä½ç½®: {final_model_dir}")
    print(f"ğŸ“‹ è®­ç»ƒé…ç½®ä¿å­˜ä½ç½®: {config_save_path}")
    print("=" * 60)
    print()
    print("ğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. LoRAæƒé‡å·²ä¿å­˜ï¼Œå¯ä»¥ä¸åŸå§‹æ¨¡å‹åˆå¹¶ä½¿ç”¨")
    print("2. æŒ‡å®šçš„RoPEå±‚å·²è¢«ç¦ç”¨")
    print("3. å¯ä»¥ä½¿ç”¨PEFTåº“åŠ è½½LoRAæƒé‡è¿›è¡Œæ¨ç†")


if __name__ == "__main__":
    main()