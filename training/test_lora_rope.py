#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•LoRA + RoPEè®­ç»ƒè„šæœ¬çš„åŠŸèƒ½
éªŒè¯æ¨¡å‹åŠ è½½ã€RoPE patchå’ŒLoRAé…ç½®æ˜¯å¦æ­£ç¡®
"""

import os
import sys
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / 'utils'))
sys.path.append(str(Path(__file__).parent))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training

from config_manager import ConfigManager
from patch_qwen_rope import patch_qwen_rope


def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½å’ŒRoPE patch"""
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½å’ŒRoPE patch...")
    
    # ä½¿ç”¨å°æ¨¡å‹è¿›è¡Œæµ‹è¯•
    model_name = "Qwen/Qwen2.5-0.5B"  # ä½¿ç”¨æ›´å°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
    
    try:
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•RoPE patch
        no_rope_layers = [0, 1, 2]  # æµ‹è¯•å‰3å±‚
        patch_qwen_rope(model, no_rope_layers=no_rope_layers)
        print(f"âœ… RoPE patchåº”ç”¨æˆåŠŸï¼Œç¦ç”¨å±‚: {no_rope_layers}")
        
        # éªŒè¯patchæ˜¯å¦ç”Ÿæ•ˆ
        for idx, layer in enumerate(model.model.layers):
            if idx in no_rope_layers:
                if hasattr(layer.self_attn, '_rope_disabled') and layer.self_attn._rope_disabled:
                    print(f"âœ… å±‚ {idx} RoPEå·²ç¦ç”¨")
                else:
                    print(f"âŒ å±‚ {idx} RoPEç¦ç”¨å¤±è´¥")
            else:
                if not hasattr(layer.self_attn, '_rope_disabled') or not layer.self_attn._rope_disabled:
                    print(f"âœ… å±‚ {idx} RoPEä¿æŒå¯ç”¨")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None


def test_lora_config():
    """æµ‹è¯•LoRAé…ç½®"""
    print("\nğŸ§ª æµ‹è¯•LoRAé…ç½®...")
    
    model, tokenizer = test_model_loading()
    if model is None:
        return False
    
    try:
        # å‡†å¤‡æ¨¡å‹è¿›è¡ŒLoRAè®­ç»ƒ
        model = prepare_model_for_kbit_training(model)
        print("âœ… æ¨¡å‹å‡†å¤‡å®Œæˆ")
        
        # åˆ›å»ºLoRAé…ç½®
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,  # ä½¿ç”¨è¾ƒå°çš„rankè¿›è¡Œæµ‹è¯•
            lora_alpha=16,
            lora_dropout=0.1,
            target_modules=[
                "q_proj",
                "k_proj", 
                "v_proj",
                "o_proj",
                "gate_proj",
                "up_proj",
                "down_proj",
            ],
            bias="none",
        )
        print("âœ… LoRAé…ç½®åˆ›å»ºæˆåŠŸ")
        
        # åº”ç”¨LoRA
        model = get_peft_model(model, lora_config)
        print("âœ… LoRAåº”ç”¨æˆåŠŸ")
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
        model.print_trainable_parameters()
        
        return True
        
    except Exception as e:
        print(f"âŒ LoRAé…ç½®å¤±è´¥: {e}")
        return False


def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•é…ç½®åŠ è½½...")
    
    config_file = "simple_config.json"
    if not Path(config_file).exists():
        print("âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡é…ç½®æµ‹è¯•")
        return True
    
    try:
        config_manager = ConfigManager(config_file)
        config = config_manager.get_config()
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥å…³é”®é…ç½®é¡¹
        if hasattr(config.model, 'no_rope_layers'):
            print(f"âœ… RoPEç¦ç”¨å±‚é…ç½®: {config.model.no_rope_layers}")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°RoPEç¦ç”¨å±‚é…ç½®")
        
        if hasattr(config.model, 'model_name'):
            print(f"âœ… æ¨¡å‹åç§°: {config.model.model_name}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False


def test_import_dependencies():
    """æµ‹è¯•ä¾èµ–å¯¼å…¥"""
    print("ğŸ§ª æµ‹è¯•ä¾èµ–å¯¼å…¥...")
    
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        import transformers
        print(f"âœ… Transformersç‰ˆæœ¬: {transformers.__version__}")
        
        import peft
        print(f"âœ… PEFTç‰ˆæœ¬: {peft.__version__}")
        
        if torch.cuda.is_available():
            print(f"âœ… CUDAå¯ç”¨ï¼ŒGPUæ•°é‡: {torch.cuda.device_count()}")
        else:
            print("âš ï¸ CUDAä¸å¯ç”¨")
        
        return True
        
    except ImportError as e:
        print(f"âŒ ä¾èµ–å¯¼å…¥å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§ª LoRA + RoPE è®­ç»ƒè„šæœ¬æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•ä¾èµ–å¯¼å…¥
    if not test_import_dependencies():
        print("âŒ ä¾èµ–æµ‹è¯•å¤±è´¥ï¼Œé€€å‡º")
        return
    
    # æµ‹è¯•é…ç½®åŠ è½½
    if not test_config_loading():
        print("âŒ é…ç½®æµ‹è¯•å¤±è´¥ï¼Œé€€å‡º")
        return
    
    # æµ‹è¯•LoRAé…ç½®
    if not test_lora_config():
        print("âŒ LoRAæµ‹è¯•å¤±è´¥ï¼Œé€€å‡º")
        return
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("âœ… LoRA + RoPE è®­ç»ƒè„šæœ¬åŠŸèƒ½æ­£å¸¸")
    print("=" * 60)


if __name__ == "__main__":
    main()