#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†åŠ è½½æµ‹è¯•è„šæœ¬

æ­¤è„šæœ¬ç”¨äºæµ‹è¯•æ•°æ®é›†åŠ è½½åŠŸèƒ½ï¼ŒéªŒè¯HuggingFaceæ•°æ®é›†åŠ è½½çš„å…¼å®¹æ€§å’Œé”™è¯¯å¤„ç†æœºåˆ¶ã€‚
å¯ä»¥åœ¨ä¸å¯åŠ¨å®Œæ•´è®­ç»ƒçš„æƒ…å†µä¸‹éªŒè¯æ•°æ®é›†åŠ è½½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

ä½¿ç”¨æ–¹æ³•:
    python test_dataset_loading.py --dataset_name "dataset_name" --text_column "text"
    python test_dataset_loading.py --config_file "path/to/config.json"
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
from typing import Optional, Dict, Any

# æ·»åŠ utilsç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'utils'))

from dataset_loader import DatasetLoader, DatasetLoadingError
from environment_adapter import EnvironmentAdapter
from transformers import AutoTokenizer

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)


def load_config_from_file(config_file: str) -> Dict[str, Any]:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
        return config
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        raise


def test_tokenizer_loading(model_name: str) -> Optional[object]:
    """æµ‹è¯•åˆ†è¯å™¨åŠ è½½"""
    try:
        logger.info(f"æµ‹è¯•åˆ†è¯å™¨åŠ è½½: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            padding_side="right"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        logger.info("âœ“ åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        logger.info(f"  - è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        logger.info(f"  - PAD token: {tokenizer.pad_token}")
        logger.info(f"  - EOS token: {tokenizer.eos_token}")
        
        return tokenizer
        
    except Exception as e:
        logger.error(f"âœ— åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
        return None


def test_dataset_loading(dataset_name: str, 
                        dataset_config: Optional[str] = None,
                        dataset_split: str = "train",
                        text_column: str = "text",
                        max_samples: int = 1000,
                        cache_dir: Optional[str] = None,
                        use_cpu: bool = False) -> bool:
    """æµ‹è¯•æ•°æ®é›†åŠ è½½"""
    try:
        logger.info(f"æµ‹è¯•æ•°æ®é›†åŠ è½½: {dataset_name}")
        logger.info(f"  - é…ç½®: {dataset_config}")
        logger.info(f"  - åˆ†å‰²: {dataset_split}")
        logger.info(f"  - æ–‡æœ¬åˆ—: {text_column}")
        logger.info(f"  - æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
        logger.info(f"  - CPUæ¨¡å¼: {use_cpu}")
        
        # åˆå§‹åŒ–æ•°æ®é›†åŠ è½½å™¨
        dataset_loader = DatasetLoader(
            cache_dir=cache_dir,
            use_cpu=use_cpu
        )
        
        # åŠ è½½æ•°æ®é›†
        dataset = dataset_loader.load_dataset_with_fallback(
            dataset_name=dataset_name,
            dataset_config=dataset_config,
            dataset_split=dataset_split,
            text_column=text_column,
            dataset_size=max_samples
        )
        
        if dataset is None:
            logger.error("âœ— æ•°æ®é›†åŠ è½½å¤±è´¥")
            return False
            
        logger.info("âœ“ æ•°æ®é›†åŠ è½½æˆåŠŸ")
        logger.info(f"  - æ•°æ®é›†å¤§å°: {len(dataset)}")
        logger.info(f"  - åˆ—å: {dataset.column_names}")
        
        # æ£€æŸ¥æ–‡æœ¬åˆ—
        if text_column in dataset.column_names:
            logger.info(f"âœ“ æ‰¾åˆ°æ–‡æœ¬åˆ—: {text_column}")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬
            logger.info("å‰3ä¸ªæ ·æœ¬:")
            for i, example in enumerate(dataset.select(range(min(3, len(dataset))))):
                text = example[text_column]
                preview = text[:100] + "..." if len(text) > 100 else text
                logger.info(f"  æ ·æœ¬ {i+1}: {preview}")
        else:
            logger.warning(f"âœ— æœªæ‰¾åˆ°æ–‡æœ¬åˆ—: {text_column}")
            logger.info(f"å¯ç”¨åˆ—: {dataset.column_names}")
            
        return True
        
    except DatasetLoadingError as e:
        logger.error(f"âœ— æ•°æ®é›†åŠ è½½é”™è¯¯: {e}")
        return False
    except Exception as e:
        logger.error(f"âœ— æœªçŸ¥é”™è¯¯: {e}")
        return False


def test_dataset_preprocessing(dataset_name: str,
                             tokenizer,
                             text_column: str = "text",
                             max_seq_length: int = 512,
                             max_samples: int = 100,
                             cache_dir: Optional[str] = None,
                             use_cpu: bool = False) -> bool:
    """æµ‹è¯•æ•°æ®é›†é¢„å¤„ç†"""
    try:
        logger.info("æµ‹è¯•æ•°æ®é›†é¢„å¤„ç†...")
        
        # åˆå§‹åŒ–æ•°æ®é›†åŠ è½½å™¨
        dataset_loader = DatasetLoader(
            cache_dir=cache_dir,
            use_cpu=use_cpu
        )
        
        # åŠ è½½æ•°æ®é›†
        dataset = dataset_loader.load_dataset_with_fallback(
            dataset_name=dataset_name,
            text_column=text_column,
            dataset_size=max_samples
        )
        
        if dataset is None:
            logger.error("âœ— æ•°æ®é›†åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œé¢„å¤„ç†æµ‹è¯•")
            return False
            
        # é¢„å¤„ç†æ•°æ®é›†
        processed_dataset = dataset_loader.preprocess_dataset(
            dataset=dataset,
            tokenizer=tokenizer,
            text_column=text_column,
            max_seq_length=max_seq_length,
            num_workers=1  # æµ‹è¯•æ—¶ä½¿ç”¨å•è¿›ç¨‹
        )
        
        if processed_dataset is None:
            logger.error("âœ— æ•°æ®é›†é¢„å¤„ç†å¤±è´¥")
            return False
            
        logger.info("âœ“ æ•°æ®é›†é¢„å¤„ç†æˆåŠŸ")
        logger.info(f"  - é¢„å¤„ç†åå¤§å°: {len(processed_dataset)}")
        logger.info(f"  - åˆ—å: {processed_dataset.column_names}")
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬
        if len(processed_dataset) > 0:
            sample = processed_dataset[0]
            logger.info(f"  - input_idsé•¿åº¦: {len(sample['input_ids'])}")
            logger.info(f"  - attention_maské•¿åº¦: {len(sample['attention_mask'])}")
            
        return True
        
    except Exception as e:
        logger.error(f"âœ— æ•°æ®é›†é¢„å¤„ç†é”™è¯¯: {e}")
        return False


def test_environment_adapter() -> bool:
    """æµ‹è¯•ç¯å¢ƒé€‚é…å™¨"""
    try:
        logger.info("æµ‹è¯•ç¯å¢ƒé€‚é…å™¨...")
        
        # åˆå§‹åŒ–ç¯å¢ƒé€‚é…å™¨
        env_adapter = EnvironmentAdapter()
        
        logger.info("âœ“ ç¯å¢ƒé€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"  - ä½¿ç”¨CPU: {env_adapter.env_info.use_cpu}")
        logger.info(f"  - CUDAå¯ç”¨: {env_adapter.env_info.cuda_available}")
        logger.info(f"  - GPUæ•°é‡: {env_adapter.env_info.gpu_count}")
        logger.info(f"  - æ€»å†…å­˜: {env_adapter.env_info.total_memory_gb:.1f} GB")
        logger.info(f"  - CPUæ ¸å¿ƒæ•°: {env_adapter.env_info.cpu_count}")
        
        # æµ‹è¯•æ¨èé…ç½®
        recommendations = env_adapter.get_recommended_config()
        logger.info("æ¨èé…ç½®:")
        logger.info(f"  - æ‰¹æ¬¡å¤§å°: {recommendations['batch_size']}")
        logger.info(f"  - å·¥ä½œè¿›ç¨‹æ•°: {recommendations['num_workers']}")
        logger.info(f"  - ç²¾åº¦: {recommendations['precision']}")
        
        return True
        
    except Exception as e:
        logger.error(f"âœ— ç¯å¢ƒé€‚é…å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®é›†åŠ è½½æµ‹è¯•è„šæœ¬")
    parser.add_argument("--config_file", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dataset_name", type=str, default="togethercomputer/RedPajama-Data-1T-Sample", help="æ•°æ®é›†åç§°")
    parser.add_argument("--dataset_config", type=str, help="æ•°æ®é›†é…ç½®")
    parser.add_argument("--dataset_split", type=str, default="train", help="æ•°æ®é›†åˆ†å‰²")
    parser.add_argument("--text_column", type=str, default="text", help="æ–‡æœ¬åˆ—åç§°")
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen2.5-3B", help="æ¨¡å‹åç§°")
    parser.add_argument("--max_samples", type=int, default=1000, help="æœ€å¤§æ ·æœ¬æ•°")
    parser.add_argument("--max_seq_length", type=int, default=512, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--cache_dir", type=str, help="ç¼“å­˜ç›®å½•")
    parser.add_argument("--force_cpu", action="store_true", help="å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
    parser.add_argument("--skip_preprocessing", action="store_true", help="è·³è¿‡é¢„å¤„ç†æµ‹è¯•")
    
    args = parser.parse_args()
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
    if args.config_file:
        try:
            config = load_config_from_file(args.config_file)
            
            # ä»é…ç½®æ–‡ä»¶ä¸­æå–æ•°æ®å‚æ•°
            data_args = config.get("data_args", {})
            model_args = config.get("model_args", {})
            
            args.dataset_name = data_args.get("dataset_name", args.dataset_name)
            args.dataset_config = data_args.get("dataset_config", args.dataset_config)
            args.dataset_split = data_args.get("dataset_split", args.dataset_split)
            args.text_column = data_args.get("text_column", args.text_column)
            args.model_name = model_args.get("model_name_or_path", args.model_name)
            args.max_seq_length = data_args.get("max_seq_length", args.max_seq_length)
            args.cache_dir = data_args.get("cache_dir", args.cache_dir)
            
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return 1
    
    logger.info("=" * 60)
    logger.info("æ•°æ®é›†åŠ è½½æµ‹è¯•å¼€å§‹")
    logger.info("=" * 60)
    
    success_count = 0
    total_tests = 0
    
    # æµ‹è¯•1: ç¯å¢ƒé€‚é…å™¨
    total_tests += 1
    logger.info("\n" + "=" * 40)
    logger.info("æµ‹è¯•1: ç¯å¢ƒé€‚é…å™¨")
    logger.info("=" * 40)
    if test_environment_adapter():
        success_count += 1
    
    # æµ‹è¯•2: åˆ†è¯å™¨åŠ è½½
    total_tests += 1
    logger.info("\n" + "=" * 40)
    logger.info("æµ‹è¯•2: åˆ†è¯å™¨åŠ è½½")
    logger.info("=" * 40)
    tokenizer = test_tokenizer_loading(args.model_name)
    if tokenizer is not None:
        success_count += 1
    
    # æµ‹è¯•3: æ•°æ®é›†åŠ è½½
    total_tests += 1
    logger.info("\n" + "=" * 40)
    logger.info("æµ‹è¯•3: æ•°æ®é›†åŠ è½½")
    logger.info("=" * 40)
    if test_dataset_loading(
        dataset_name=args.dataset_name,
        dataset_config=args.dataset_config,
        dataset_split=args.dataset_split,
        text_column=args.text_column,
        max_samples=args.max_samples,
        cache_dir=args.cache_dir,
        use_cpu=args.force_cpu
    ):
        success_count += 1
    
    # æµ‹è¯•4: æ•°æ®é›†é¢„å¤„ç†ï¼ˆå¦‚æœåˆ†è¯å™¨åŠ è½½æˆåŠŸä¸”æœªè·³è¿‡ï¼‰
    if tokenizer is not None and not args.skip_preprocessing:
        total_tests += 1
        logger.info("\n" + "=" * 40)
        logger.info("æµ‹è¯•4: æ•°æ®é›†é¢„å¤„ç†")
        logger.info("=" * 40)
        if test_dataset_preprocessing(
            dataset_name=args.dataset_name,
            tokenizer=tokenizer,
            text_column=args.text_column,
            max_seq_length=args.max_seq_length,
            max_samples=min(args.max_samples, 100),  # é¢„å¤„ç†æµ‹è¯•ä½¿ç”¨è¾ƒå°‘æ ·æœ¬
            cache_dir=args.cache_dir,
            use_cpu=args.force_cpu
        ):
            success_count += 1
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯•ç»“æœæ±‡æ€»")
    logger.info("=" * 60)
    logger.info(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    logger.info(f"æˆåŠŸæµ‹è¯•æ•°: {success_count}")
    logger.info(f"å¤±è´¥æµ‹è¯•æ•°: {total_tests - success_count}")
    logger.info(f"æˆåŠŸç‡: {success_count/total_tests*100:.1f}%")
    
    if success_count == total_tests:
        logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ•°æ®é›†åŠ è½½åŠŸèƒ½æ­£å¸¸ã€‚")
        return 0
    else:
        logger.error(f"\nâŒ {total_tests - success_count} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        return 1


if __name__ == "__main__":
    exit(main())