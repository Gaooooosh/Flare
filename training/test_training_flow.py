#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å®Œæ•´çš„LoRAè®­ç»ƒæµç¨‹
"""

import os
import sys
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / 'utils'))

import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from datasets import Dataset

def create_dummy_dataset(tokenizer, size=100):
    """åˆ›å»ºè™šæ‹Ÿæ•°æ®é›†ç”¨äºæµ‹è¯•"""
    texts = [f"This is test sentence number {i}." for i in range(size)]
    
    def tokenize_function(examples):
        return tokenizer(
            examples["text"], 
            truncation=True, 
            padding=False, 
            max_length=512
        )
    
    dataset = Dataset.from_dict({"text": texts})
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    
    return tokenized_dataset

def test_training_flow():
    """æµ‹è¯•å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    print("ğŸ§ª æµ‹è¯•LoRAè®­ç»ƒæµç¨‹...")
    
    # ä½¿ç”¨å°æ¨¡å‹
    model_name = "Qwen/Qwen2.5-0.5B"
    
    # åŠ è½½åˆ†è¯å™¨
    print("ğŸ“¦ åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
    )
    
    # å‡†å¤‡æ¨¡å‹
    print("âš™ï¸ å‡†å¤‡æ¨¡å‹...")
    model = prepare_model_for_kbit_training(model)
    
    # åº”ç”¨LoRA
    print("ğŸ¯ åº”ç”¨LoRA...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        bias="none",
    )
    model = get_peft_model(model, lora_config)
    
    # éªŒè¯æ¢¯åº¦
    print("ğŸ” éªŒè¯æ¢¯åº¦è®¾ç½®...")
    trainable_count = sum(1 for p in model.parameters() if p.requires_grad)
    total_count = sum(1 for p in model.parameters())
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_count}/{total_count}")
    
    if trainable_count == 0:
        print("âŒ æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼")
        return False
    
    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ“Š åˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
    train_dataset = create_dummy_dataset(tokenizer, 50)
    eval_dataset = create_dummy_dataset(tokenizer, 10)
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )
    
    # è®­ç»ƒå‚æ•°
    print("âš™ï¸ è®¾ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        output_dir="./test_output",
        num_train_epochs=1,
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=1,
        learning_rate=1e-4,
        warmup_steps=5,
        logging_steps=5,
        save_steps=50,
        eval_steps=50,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=False,
        report_to=None,
        dataloader_pin_memory=False,
        dataloader_num_workers=0,
        bf16=True,
        gradient_checkpointing=True,
        max_grad_norm=1.0,
        remove_unused_columns=True,
        save_only_model=True,
        max_steps=10,  # åªè®­ç»ƒ10æ­¥ç”¨äºæµ‹è¯•
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("ğŸ—ï¸ åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    # æµ‹è¯•è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹æµ‹è¯•è®­ç»ƒ...")
    try:
        trainer.train()
        print("âœ… è®­ç»ƒæµ‹è¯•æˆåŠŸï¼")
        return True
    except Exception as e:
        print(f"âŒ è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_training_flow()
    if success:
        print("\nğŸ‰ LoRAè®­ç»ƒæµç¨‹æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nğŸ’¥ LoRAè®­ç»ƒæµç¨‹æµ‹è¯•å¤±è´¥ï¼")